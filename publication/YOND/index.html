<!DOCTYPE html>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>YOND - You Only Need a Denoiser</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> 
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  </head>

  <style>
  .section-title {
    font-family: "monospace"
  }
  .authors {
    font-family: "monospace";
  }
  h1, h2, h3, h4, h5, h6, p {
    font-family: "monospace";
  }
  </style>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-4">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h1 style="font-family: Lato;">YOND: Practical Blind Raw Image Denoising Free from Camera-Specific Data Dependency</h1>
            <h4 style="color:#5a6268;">TPAMI Minor Revision</h4>
            <hr>
            <h5> <a href="https://fenghansen.github.io/" target="_blank">Hansen Feng</a><sup>1</sup>, 
              <a href="https://wang-lizhi.github.io/" target="_blank">Lizhi Wang</a><sup>1,2</sup>, 
                Yiqi Huang<sup>1</sup>, Tong Li<sup>1</sup>, Lin Zhu<sup>1</sup>, Hua Huang<sup>2</sup>
            </h5>
            <p style="font-size: large; font-weight: 300;">
              <sup>1</sup>Beijing Institute of Technology &nbsp;&nbsp;
              <sup>2</sup>Beijing Normal University &nbsp;&nbsp; 
            </p>
            <div class="row justify-content-center">
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2506.03645" role="button"  target="_blank">
                  <i class="fa fa-file"></i> Paper (Arxiv)</a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/fenghansen/YOND_public" role="button"  target="_blank">
                  <i class="fa fa-github"></i> Code (GitHub)</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://pan.baidu.com/s/1nox4vMXwhpsIHG6qri5SIg?pwd=vmcl" role="button">
                    <i class="fa fa-cloud-download"></i> Resources</a> </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Evaluation Guidelines</h3>
            <hr style="margin-top:0px">
            <ul class="text-left">
              <li>We have launched an online demo on <a href="https://huggingface.co/spaces/hansen97/YOND">Hugging Face Space</a>!</li>
              <li>YOND's core modules (CNE & EM-VST) have been temporarily obfuscated with PyArmor to comply with recent laboratory confidentiality regulations. You can still run inference, but the source code will remain encrypted until the paper is officially accepted.</li>
              <li>Complete experimental results (visualized as RGB images) are available at <a href="https://pan.baidu.com/s/1nox4vMXwhpsIHG6qri5SIg?pwd=vmcl#list/path=%2F">Resources</a>. This includes YOND's inference results on four public datasets (results) and crops used for comparison in the manuscript (crops@paper).</li>
            </ul><!-- <img class="img-fluid" src="./images/pipeline.png" alt="teaser" width="90%"> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
          <p class="text-justify">
            The rapid advancement of photography has created a growing demand for a practical blind raw image denoising method. Recently, learning-based methods have become mainstream due to their excellent performance. However, most existing learning-based methods suffer from camera-specific data dependency, resulting in performance drops when applied to data from unknown cameras.
            To address this challenge, we introduce a novel blind raw image denoising method named YOND, which represents You Only Need a Denoiser. Trained solely on synthetic data, YOND can generalize robustly to noisy raw images captured by diverse unknown cameras.
            Specifically, we propose three key modules to guarantee the practicality of YOND: coarse-to-fine noise estimation (CNE), expectation-matched variance-stabilizing transform (EM-VST), and SNR-guided denoiser (SNR-Net).
            Firstly, we propose CNE to identify the camera noise characteristic, refining the estimated noise parameters based on the coarse denoised image.
            Secondly, we propose EM-VST to eliminate camera-specific data dependency, correcting the bias expectation of VST according to the noisy image.
            Finally, we propose SNR-Net to offer controllable raw image denoising, supporting adaptive adjustments and manual fine-tuning.
            Extensive experiments on unknown cameras, along with flexible solutions for challenging cases, demonstrate the superior practicality of our method.
          </p>
          <img class="img-fluid" src="./images/YOND_teaser.png" alt="teaser" width="90%">
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Performance</h3>
            <hr style="margin-top:0px">
            <p class="text-justify" style="font-family: Lato; font-size: x-large; font-weight: 500;">Comparison</p>
            <h5>Quantitative Results</h5>
            <img class="img-fluid" src="./images/tab_SIDD.png" width="100%">
            <p class="text-justify">
              The comparison of different self-supervised methods on SIDD dataset and DND dataset.
            </p>
            <details><summary>Click to get details</summary>
            <br>
            <h5>SIDD dataset and DND dataset</h5>
            <img class="img-fluid" src="./images/results_SIDD.png" width="100%">
            <br><br>
            <img class="img-fluid" src="./images/results_DND.png" width="100%">
            <p class="text-justify">
              Blind raw image denoising results on images from the DND dataset. 
            </p>
            </details>
        </div>
        
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Discussion</h3>
            <hr style="margin-top:0px">
            <p class="text-justify">
              The default settings enable YOND to outperform existing methods in most cases, while handling challenging cases is particularly essential in practice. A denoising method should be controllable to fulfill the slogan of ``You Only Need a Denoiser". Breaking data dependency has unlocked the powerful interactivity of YOND, allowing manual adjustments of various parameters to handle challenging cases. Next, we will we introduce two solutions to expand the applicability of YOND beyond its default settings.
            </p>
            <p class="text-justify" style="font-family: Lato; font-size: x-large; font-weight: 500;">Solutions Based on Fine-tuning</p>
            <img class="img-fluid" src="./images/discuss_finetune.png">
            <br><br>
            <p class="text-justify" style="font-family: Lato; font-size: x-large; font-weight: 500;">Solutions Based on YOND-p</p>
            <img class="img-fluid" src="./images/discuss_YOND-p.png">
            </details>
        </div>
        
      </div>
    </div>
  </section>

  <!-- citing -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{huang2021vs-net,
  title={VS-Net: Voting with Segmentation for Visual Localization},
  author={Huang, Zhaoyang and Zhou, Han and Li, Yijin and Yang, Bangbang and Xu, Yan and Zhou, Xiaowei and Bao, Hujun and Zhang, Guofeng and Li, Hongsheng},
  booktitle={CVPR},
  year={2021}
}</code></pre>
          <hr>
      </div>
    </div>
  </div> -->
  <footer class="text-center" style="margin-bottom:10px">
    <br>
      <p style="text-align:center;font-size:small;">
        Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template. <br>
        本站访客数<span id="busuanzi_value_site_uv"></span>人次<br>
        本文总阅读量<span id="busuanzi_value_page_pv"></span>次
      </p>
  </footer>

</body>
</html>
