<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Learnability Enhancement for Low-light Raw Denoising: A Data Perspective</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> 
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  </head>

  <style>
  .section-title {
    font-family: "monospace"
  }
  .authors {
    font-family: "monospace";
  }
  h1, h2, h3, h4, h5, h6, p {
    font-family: "monospace";
  }
  </style>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-4">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2 style="font-family: Lato;">Learnability Enhancement for Low-light Raw Denoising: A Data Perspective</h2->
            <h4 style="color:#5a6268;">ACMMM 2022 (Best Paper Runner-Up Award) / TPAMI (Under Review)</h4>
            <hr>
            <h6> <a href="https://fenghansen.github.io/" target="_blank">Hansen Feng</a><sup>1</sup>, 
              <a href="https://wang-lizhi.github.io/" target="_blank"></a>Lizhi Wang<sup>1</sup>, 
                Yuzhi Wang<sup>2</sup>, Haoqiang Fan<sup>2</sup>, Hua Huang<sup>3</sup>
            </h6>
            <p>
                <sup>1</sup>Beijing Institute of Technology &nbsp;&nbsp;
                <sup>2</sup>Megvii Technology &nbsp;&nbsp;  <br>
                <sup>3</sup>Beijing Normal University &nbsp;&nbsp; 
                </p>
            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2207.06103" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper (ACMMM)</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/megvii-research/PMN" role="button"  target="_blank">
                    <i class="fa fa-github-alt"></i> Code (ACMMM)</a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/megvii-research/PMN/tree/TPAMI" role="button"  target="_blank">
                  <i class="fa fa-github-alt"></i> Code (TPAMI)</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://pan.baidu.com/s/1fXlb-Q_ofHOtVOufe5cwDg?pwd=vmcl " role="button">
                    <i class="fa fa-database"></i> Data</a> </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
          <p class="text-justify">Low-light raw image denoising is an important and valuable task in computational photography where learning-based methods trained with paired real data are mainstream.
            However, the limited data volume, complicated noise distribution, and underdeveloped data quality have constituted the learnability bottleneck of paired real data, which limits the denoising performance of learning-based methods.
            To break through the bottleneck, we introduce a learnability enhancement strategy for low-light raw image denoising from a data perspective. We reform paired real data and image acquisition protocol according to noise modeling.
            Our strategy includes two efficient techniques: shot noise augmentation (SNA) and dark shading correction (DSC).
            SNA improves the precision of data mapping by increasing the data volume, and DSC reduces the complexity of data mapping by reducing the noise complexity.
            To improve the data quality, we finally propose a new image acquisition protocol and dataset with high learnability.
            Extensive experiments on public datasets and our dataset demonstrate the superiority of our learnability enhancement strategy on low-light raw image denoising. 
          <iframe class="img-fluid" src="./images/teaser.pdf" alt="teaser" width="50%">
          <p class="text-justify">From the data perspective, image denoising via learning-based methods can be modeled as a data mapping from the noisy image to the clean image. The learnability of data mapping depends on the complexity of noise distribution, the volume of paired data, and the quality of labeled data. Accordingly, we develop a learnability enhancement strategy for low-light raw image denoising by reforming paired real data according to noise modeling.</p>
          <p class="text-justify">
            Our main contributions are summarized as follows:
            <ol>
              <li style="text-align: left;font-family: Lato;">We light the idea of learnability enhancement for low-light raw image denoising by reforming paired real data according to the noise model.</li> 
              <li style="text-align: left;font-family: Lato;">We increase the data volume of paired real data with a novel shot noise augmentation method, which promotes the precision of data mapping through decoupling the real noise into shot noise and read noise.</li>
              <li style="text-align: left;font-family: Lato;">We reduce the complexity of the real noise model with a novel dark shading correction method, which reduces the complexity of data mapping through decoupling the read noise into temporal stable noise and temporal variant noise.</li>
              <li style="text-align: left;font-family: Lato;">We develop a high-quality image acquisition protocol and collect a low-light raw image denoising dataset, which promotes the reliability of data mapping through improving the data quality on paired real data.</li>
              <li style="text-align: left;font-family: Lato;">We demonstrate the superior performance of our methods on public datasets and our dataset in both quantitative results and visual quality.</li>
            <ol>
          </p>
          </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Method</h3>
            <hr style="margin-top:0px">
            <p class="text-justify" style="font-family: Lato; font-size: x-large; font-weight: 500;">Noise Formation Model</p>
            <iframe class="img-fluid" src="./images/nfm.pdf" alt="method" width="70%">
            <p class="text-justify">
              Overview of simplified imaging pipeline. Photons are converted into charge and sequentially voltages, then amplified, and finally quantized into digital signals. We visualize the noise and connect it with the corresponding noise sources. Noise-free scene irradiance suffers inescapable photon shot noise and read noise from various electronic components, thus the output of the sensor is a noisy image.
            </p>
            <p class="text-justify" style="font-family: Lato; font-size: x-large; font-weight: 500;">Framework</p>
            <iframe class="img-fluid" src="./images/pipeline.pdf" alt="architecture" width="90%">
            <p class="text-justify">
              Overview of our framework. For training, we first correct the dark shading hiding in the noisy raw image by DSC. Then we augment the clean image and the noisy image to obtain new data pairs by SNA. Finally, We use augmented noisy images and augmented clean images to train a neural network. For inference, we just denoise the noisy image after correcting dark shading with the trained denoising model.
            </p>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Performance</h3>
            <p class="text-justify" style="font-family: Lato; font-size: x-large; font-weight: 500;">Quantitative Results</p>
            <img class="img-fluid" src="./images/comparison0.png" width="90%">
            <p class="text-justify">
              Overview of our framework. For training, we first correct the dark shading hiding in the noisy raw image by DSC. Then we augment the clean image and the noisy image to obtain new data pairs by SNA. Finally, We use augmented noisy images and augmented clean images to train a neural network. For inference, we just denoise the noisy image after correcting dark shading with the trained denoising model.
            </p>
            <p class="text-justify" style="font-family: Lato; font-size: x-large; font-weight: 500;">Ablation Study</p>
            <iframe class="img-fluid" src="./images/ablation.pdf" width="90%">
            <p class="text-justify">
              Ablation study of different learnability enhancement modules on the ELD dataset, SID dataset, and LRID Dataset. ``*" indicates that the module uses the implementation from the preliminary version.
            </p>
            <img class="img-fluid" src="./images/ablation.png" width="90%">
            <p class="text-justify">
              Representative visual result comparison of different data schemes. Our full learnability enhancement strategy (Paired + SNA + DSC) promotes more exact color and clearer details compared to other baselines.
            </p>
        </div>
        
      </div>
    </div>
  </section>

  <!-- citing -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{huang2021vs-net,
  title={VS-Net: Voting with Segmentation for Visual Localization},
  author={Huang, Zhaoyang and Zhou, Han and Li, Yijin and Yang, Bangbang and Xu, Yan and Zhou, Xiaowei and Bao, Hujun and Zhang, Guofeng and Li, Hongsheng},
  booktitle={CVPR},
  year={2021}
}</code></pre>
          <hr>
      </div>
    </div>
  </div> -->

  <footer class="text-center" style="margin-bottom:10px">
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
  </footer>

</body>
</html>
